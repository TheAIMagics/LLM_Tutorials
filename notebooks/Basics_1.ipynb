{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1690716738222,"user":{"displayName":"satya thakur","userId":"14785212108924463454"},"user_tz":-330},"id":"XYsLdGURe2GU"},"outputs":[],"source":["import os\n","\n","OPENAI_API_KEY=os.environ['OPENAI_API_KEY'] "]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10204,"status":"ok","timestamp":1690716748418,"user":{"displayName":"satya thakur","userId":"14785212108924463454"},"user_tz":-330},"id":"bUlQn-AyfNKm","outputId":"6d7bc033-2101-4c8a-a08f-95fd1f554855"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install langchain openai -q"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":2017,"status":"ok","timestamp":1690716750428,"user":{"displayName":"satya thakur","userId":"14785212108924463454"},"user_tz":-330},"id":"6KVBFiJLhwtF"},"outputs":[],"source":["from langchain.llms import OpenAI"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1690716750429,"user":{"displayName":"satya thakur","userId":"14785212108924463454"},"user_tz":-330},"id":"7MCWZDzuh4Pi"},"outputs":[],"source":["llm = OpenAI(openai_api_key=OPENAI_API_KEY, temperature=0.9)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"elapsed":2305,"status":"ok","timestamp":1690716752729,"user":{"displayName":"satya thakur","userId":"14785212108924463454"},"user_tz":-330},"id":"jrNcDR5YiCAm","outputId":"496ed9c8-deb8-4907-a768-73b2e400056c"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n\\nLLM stands for \"Legum Magister\" and is an advanced postgraduate academic degree in law. It is typically a one-year program for those who have already completed a first law degree at an accredited institution. The LLM program usually includes a combination of academic coursework and a research paper or thesis, focusing on a specific area of law. In the US, LLM programs are offered by many top law schools and the degree opens up career opportunities in the field of law.'"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["llm(\"What is LLM\")"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1690716752736,"user":{"displayName":"satya thakur","userId":"14785212108924463454"},"user_tz":-330},"id":"47RildAGjc6U"},"outputs":[],"source":["from langchain.schema import AIMessage,HumanMessage,SystemMessage\n","from langchain.chat_models import ChatOpenAI"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1690716752738,"user":{"displayName":"satya thakur","userId":"14785212108924463454"},"user_tz":-330},"id":"JJXtgBYIjmh-"},"outputs":[],"source":["chat = ChatOpenAI(model_name = \"gpt-3.5-turbo\", temperature=0.3,openai_api_key=OPENAI_API_KEY)\n","messages = [\n","    SystemMessage(content=\"You are an expert data scientist\"),\n","    HumanMessage(content = \"Write a Python script that trains a neural network on simulated data \")\n","]"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":12106,"status":"ok","timestamp":1690716764818,"user":{"displayName":"satya thakur","userId":"14785212108924463454"},"user_tz":-330},"id":"_wv67g6HkSBz"},"outputs":[],"source":["response = chat(messages)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1690716764819,"user":{"displayName":"satya thakur","userId":"14785212108924463454"},"user_tz":-330},"id":"JVD_nMJhkjSw","outputId":"f90b3a62-344a-499f-8889-4c327368bd42"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sure! Here's a simple example of a Python script that trains a neural network on simulated data using the Keras library:\n","\n","```python\n","import numpy as np\n","from keras.models import Sequential\n","from keras.layers import Dense\n","\n","# Generate simulated data\n","np.random.seed(0)\n","X = np.random.rand(100, 2)\n","y = np.random.randint(2, size=(100, 1))\n","\n","# Define the neural network model\n","model = Sequential()\n","model.add(Dense(32, input_dim=2, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X, y, epochs=10, batch_size=10)\n","\n","# Evaluate the model\n","loss, accuracy = model.evaluate(X, y)\n","print(f\"Loss: {loss}\")\n","print(f\"Accuracy: {accuracy}\")\n","```\n","\n","In this script, we first generate simulated data using `numpy.random.rand` and `numpy.random.randint`. We then define a simple neural network model using the `Sequential` class from Keras and add two dense layers with ReLU and sigmoid activations, respectively. The model is then compiled with the binary cross-entropy loss and the Adam optimizer.\n","\n","Next, we train the model using the `fit` method, specifying the number of epochs and batch size. Finally, we evaluate the trained model using the `evaluate` method and print the loss and accuracy.\n"]}],"source":["print(response.content, end='\\n')"]},{"cell_type":"markdown","metadata":{"id":"0TMvpQuimBvg"},"source":["## PROMPTS IN LANGCHAIN"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1690716764819,"user":{"displayName":"satya thakur","userId":"14785212108924463454"},"user_tz":-330},"id":"I_OphuodlG7T"},"outputs":[],"source":["from langchain import PromptTemplate"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1690716764820,"user":{"displayName":"satya thakur","userId":"14785212108924463454"},"user_tz":-330},"id":"YV-VGX85lcpw"},"outputs":[],"source":["template = \"\"\"\n","You are an expert data scientist with an expertise in building deep learning models.\n","Explain the concept of {concept} in a couple of lines\n","\"\"\""]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1690716764820,"user":{"displayName":"satya thakur","userId":"14785212108924463454"},"user_tz":-330},"id":"YODPu4tclrjr"},"outputs":[],"source":["prompt = PromptTemplate(\n","    input_variables=[\"concept\"],\n","    template = template\n",")"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"elapsed":1873,"status":"ok","timestamp":1690716766687,"user":{"displayName":"satya thakur","userId":"14785212108924463454"},"user_tz":-330},"id":"MV8oQn8nkviw","outputId":"82648c1e-757a-4d7e-d5f4-66571c55e47c"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nRegularization is a technique used to improve the generalization of a model and reduce the likelihood of model overfitting by regularizing (i.e., adding constraints to) the weights and biases of a model. This is typically done by introducing a penalty term to the loss function; most commonly a L1 or L2 penalty.'"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["llm(prompt.format(concept= \"regularization\"))"]},{"cell_type":"markdown","metadata":{"id":"Ac73NH-nmH6K"},"source":["## CHAINs"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1690716820229,"user":{"displayName":"satya thakur","userId":"14785212108924463454"},"user_tz":-330},"id":"MuBCtenblNAY"},"outputs":[],"source":["from langchain.chains import LLMChain"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1690716872947,"user":{"displayName":"satya thakur","userId":"14785212108924463454"},"user_tz":-330},"id":"l6XtImLDk0-K"},"outputs":[],"source":["chain = LLMChain(llm = llm, prompt = prompt)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1257,"status":"ok","timestamp":1690716898773,"user":{"displayName":"satya thakur","userId":"14785212108924463454"},"user_tz":-330},"id":"9FjUyB8XlBvM","outputId":"e5b24404-e53a-4e35-d743-9b7edc22ba57"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Autoencoders are a type of unsupervised deep learning models used to learn efficient representations of data by compressing them and reconstructing the data from the compressed version. They are used for feature extraction and dimensionality reduction.\n"]}],"source":["print(chain.run(\"Autoencoder\"))"]},{"cell_type":"markdown","metadata":{"id":"5kIOZC6mlh3v"},"source":["#### Sequential chain of multiple chains"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1690717161400,"user":{"displayName":"satya thakur","userId":"14785212108924463454"},"user_tz":-330},"id":"e9iqb0y3lHrB"},"outputs":[],"source":["second_prompt = PromptTemplate(\n","    input_variables=[\"ml_concept\"],\n","    template = \"Turn the concept description of {ml_concept} and explain it to me like I'm five in 500 words\"\n",")\n","chain_two = LLMChain(llm = llm, prompt = second_prompt)"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1690717230005,"user":{"displayName":"satya thakur","userId":"14785212108924463454"},"user_tz":-330},"id":"mCVSH6nimIF0"},"outputs":[],"source":["from langchain.chains import SimpleSequentialChain\n","overall_chain = SimpleSequentialChain(chains = [chain, chain_two], verbose = True)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7453,"status":"ok","timestamp":1690717287508,"user":{"displayName":"satya thakur","userId":"14785212108924463454"},"user_tz":-330},"id":"ZOqPchB4mY3I","outputId":"71c1812e-e2f8-4575-97cb-2b5f99de354d"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n","\u001b[36;1m\u001b[1;3m\n","Autoencoders are unsupervised neural networks that are used to learn data representations through compression and reconstruction, by mapping the input to a low dimensional representation based on hidden layers (encoders), and then decoding it back to the original domain (decoders). They are used to identify patterns and detect outliers in data.\u001b[0m\n","\u001b[33;1m\u001b[1;3m\n","\n","Autoencoders are special kinds of computer programs that help us to learn about data. They are like a game of hide and seek, where the computer has to figure out how to store and remember data.\n","\n","To play the game, the autoencoder has a few important steps. The first step is called the encoder. This is where the data is put into a special box and is compressed into a much smaller size. It's like a magician shrinking an elephant into a suitcase! The shrunken data is then sent to a hidden layer (like a secret hideout) where it is stored. \n","\n","The next step is called the decoder. This is where the data is taken out of the hideout and made back to its original size. It's like a magician pulling an elephant out of a suitcase! The data is then sent back to us in its original form.\n","\n","Autoencoders help us to identify patterns in data. For example, if there are large numbers of duplicates in the data, the autoencoder can detect that and alert us so that we can do something about it. It can also help us to detect outliers, which are data points that don't quite fit with the rest of\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\n","Autoencoders are special kinds of computer programs that help us to learn about data. They are like a game of hide and seek, where the computer has to figure out how to store and remember data.\n","\n","To play the game, the autoencoder has a few important steps. The first step is called the encoder. This is where the data is put into a special box and is compressed into a much smaller size. It's like a magician shrinking an elephant into a suitcase! The shrunken data is then sent to a hidden layer (like a secret hideout) where it is stored. \n","\n","The next step is called the decoder. This is where the data is taken out of the hideout and made back to its original size. It's like a magician pulling an elephant out of a suitcase! The data is then sent back to us in its original form.\n","\n","Autoencoders help us to identify patterns in data. For example, if there are large numbers of duplicates in the data, the autoencoder can detect that and alert us so that we can do something about it. It can also help us to detect outliers, which are data points that don't quite fit with the rest of\n"]}],"source":["explaination = overall_chain.run(\"Autoencoders\")\n","print(explaination)"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1690717506763,"user":{"displayName":"satya thakur","userId":"14785212108924463454"},"user_tz":-330},"id":"AtDghDo5mlJl"},"outputs":[],"source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size = 200,\n","    chunk_overlap = 0\n",")"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1690717507501,"user":{"displayName":"satya thakur","userId":"14785212108924463454"},"user_tz":-330},"id":"edGHv77anNzh"},"outputs":[],"source":["texts = text_splitter.create_documents([explaination])"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1690717508247,"user":{"displayName":"satya thakur","userId":"14785212108924463454"},"user_tz":-330},"id":"IxtQSldonVLs","outputId":"248efaee-3d41-44c2-ca86-ed63ea0a916a"},"outputs":[{"data":{"text/plain":["[Document(page_content='Autoencoders are special kinds of computer programs that help us to learn about data. They are like a game of hide and seek, where the computer has to figure out how to store and remember data.', metadata={}),\n"," Document(page_content=\"To play the game, the autoencoder has a few important steps. The first step is called the encoder. This is where the data is put into a special box and is compressed into a much smaller size. It's\", metadata={}),\n"," Document(page_content='like a magician shrinking an elephant into a suitcase! The shrunken data is then sent to a hidden layer (like a secret hideout) where it is stored.', metadata={}),\n"," Document(page_content=\"The next step is called the decoder. This is where the data is taken out of the hideout and made back to its original size. It's like a magician pulling an elephant out of a suitcase! The data is\", metadata={}),\n"," Document(page_content='then sent back to us in its original form.', metadata={}),\n"," Document(page_content='Autoencoders help us to identify patterns in data. For example, if there are large numbers of duplicates in the data, the autoencoder can detect that and alert us so that we can do something about', metadata={}),\n"," Document(page_content=\"it. It can also help us to detect outliers, which are data points that don't quite fit with the rest of\", metadata={})]"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["texts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9qci9F9onh9F"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOpujZxWR+wl5jO+QFdNZwt","provenance":[]},"kernelspec":{"display_name":"llm_tutorial","language":"python","name":"llm_tutorial"}},"nbformat":4,"nbformat_minor":0}
