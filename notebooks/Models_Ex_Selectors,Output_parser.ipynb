{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nJGqh7c_CF5"
      },
      "source": [
        "Chat Messages\n",
        "Like text, but specified with a message type (System, Human, AI)\n",
        "\n",
        "- System - Helpful background context that tell the AI what to do\n",
        "- Human - Messages that are intented to represent the user\n",
        "- AI - Messages that show what the AI responded with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t027s7g4_Gr_",
        "outputId": "b31807f8-1932-4ab9-86f5-114c8be17368"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m71.7/73.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install langchain openai -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NPCNcTOM_PuH"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "SdVNeiMN_v4A"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "OPENAI_API_KEY=os.environ['OPENAI_API_KEY'] \n",
        "chat = ChatOpenAI(temperature=0.9, openai_api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wm2-eTO8_6Tn",
        "outputId": "0bc2aeb6-9d95-47b3-e896-2a8755b2de1e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='You should try a Caprese salad with fresh tomatoes, mozzarella cheese, and basil.', additional_kwargs={}, example=False)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat(\n",
        "    [\n",
        "        SystemMessage(content=\"You are a nice AI bot that helps a user figure out what to eat in one short sentence\"),\n",
        "        HumanMessage(content=\"I like tomatoes, what should I eat?\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGsygTWOAnHF",
        "outputId": "0ce6dae4-5068-481f-fbaf-a14ca5a00cb1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Mon, 31 Jul 2023 08:29:13 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7ef471550d5744f3-ATL', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Explore ancient temples, indulge in flavorful cuisine, and take a serene houseboat ride through Kerala's backwaters.\", additional_kwargs={}, example=False)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat(\n",
        "    [\n",
        "        SystemMessage(content=\"You are a nice AI bot that helps a user figure out where to travel in one short sentence\"),\n",
        "        HumanMessage(content=\"I like the beaches where should I go?\"),\n",
        "        AIMessage(content=\"You should go to India\"),\n",
        "        HumanMessage(content=\"What else should I do when I'm there?\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Q4CJBF0EaUW"
      },
      "source": [
        "### Models = Interface to AI brains\n",
        "#### 1. language Model\n",
        "A model that does text in and text out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "XjwP2vAYDPYZ"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(model_name=\"text-ada-001\", openai_api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Agt8KhsYEu8r",
        "outputId": "06a4ca2c-231f-4994-e67d-8fda0fb9c010"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\nSaturday'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm(\"What day comes after Friday?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZHqEm1oSIFR"
      },
      "source": [
        "#### 2. Chat Model\n",
        "A model that takes a series of messages and returns a message output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "AhvRHgpGSDF6"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "2GapDVvvStzw"
      },
      "outputs": [],
      "source": [
        "chat = ChatOpenAI(temperature=1 , openai_api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4g4A2IBS1hR",
        "outputId": "ec035756-9a34-4738-92fa-1a7fce53452a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Why did the hipster burn his tongue? Because he drank his coffee before it was cool. But seriously, try taking a plane or hitching a ride on a unicorn.', additional_kwargs={}, example=False)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat(\n",
        "    [\n",
        "        SystemMessage(content=\"You are an unhelpful AI bot that makes a joke at whatever the user says\"),\n",
        "        HumanMessage(content=\"I would like to go to New York, how should I do this?\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoSTMWZsS_ux"
      },
      "source": [
        "#### 3. Embedding Model\n",
        "Change your text into vector(A series of numbers that hold semantic meaning of your text).\n",
        "- Mainly used when comparing two pieces of text together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "aCtkaqrtS3uw"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Qd3Gb-EnTbjY"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Xt9GFxkpTqfX"
      },
      "outputs": [],
      "source": [
        "text = \"Hi! It's time for the beach\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "QVKMxr9vTssA"
      },
      "outputs": [],
      "source": [
        "text_embedding = embeddings.embed_query(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arfnIdUFULvz",
        "outputId": "20b7ae46-26be-4271-bff1-af65c5740efa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1536"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(text_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_CQQcX-UeuR"
      },
      "source": [
        "#### Prompts - Text generally used as instructions to your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "m4xuvFHeUOnT"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(model_name = \"text-davinci-003\", openai_api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "9lyyvs59U3A-"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Today is Monday, tomorrow is Wednesday.\n",
        "\n",
        "What is wrong with that statement?\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "HYH9gpJ0U40_",
        "outputId": "c4eadfd8-2f86-4a39-88b4-1c400df92477"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nThe statement is incorrect; tomorrow is Tuesday.'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlPhzr9XVAsf"
      },
      "source": [
        "#### Prompt Template\n",
        "An object that helps create prompts based on a combination of user input , other non-static information and a fixed template string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "grkO4MrRU6ym"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "DUtz-2H1VfmH"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"\n",
        "I really want to travel to {location}. What should I do there?\n",
        "\n",
        "Respond in one short sentence\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "GS-CBY8HVlxv"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(input_variables=[\"location\"], template=template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "yTXTdHLQVxD_"
      },
      "outputs": [],
      "source": [
        "final_prompt = prompt.format(location=\"India\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zSA0l0xYV4kl",
        "outputId": "8e1032ff-38af-4c24-a953-25b44104a108"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Visit the Taj Mahal, explore the Golden Triangle, and take a river cruise down the Ganges.'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm(final_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA7CTSLxWHiI"
      },
      "source": [
        "#### Example Selectors\n",
        "- An easy way to select from a series of examples that allow you to dynamic place in-context information into your prompt. Often used when your task is nuanced or you have a large list of examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "6RB5RUaeV9KO"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.prompts import FewShotPromptTemplate , PromptTemplate\n",
        "from langchain.llms import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Pilb85T0W4gN"
      },
      "outputs": [],
      "source": [
        "ex_prompt = PromptTemplate(\n",
        "    input_variables= [\"input\",\"output\"],\n",
        "    template=\"Example Input: {input}\\nExample Output: {output}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "JG50kbqxXM10"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    {\"input\": \"pirate\", \"output\": \"ship\"},\n",
        "    {\"input\": \"pilot\", \"output\": \"plane\"},\n",
        "    {\"input\": \"driver\", \"output\": \"car\"},\n",
        "    {\"input\": \"tree\", \"output\": \"ground\"},\n",
        "    {\"input\": \"bird\", \"output\": \"nest\"},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyzTrT5lX9Me",
        "outputId": "bcb46891-c8b8-4ad0-b492-2dbcefc52b88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken faiss-cpu -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "3RFmJ8u1XR-R"
      },
      "outputs": [],
      "source": [
        "# SemanticSimilarityExampleSelector will select examples that are similar to your input by semantic meaning\n",
        "ex_selector = SemanticSimilarityExampleSelector.from_examples(\n",
        "    examples,\n",
        "    OpenAIEmbeddings(openai_api_key= OPENAI_API_KEY),\n",
        "    FAISS,\n",
        "    k=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "CJL5ihOzXyVf"
      },
      "outputs": [],
      "source": [
        "similar_prompt = FewShotPromptTemplate(\n",
        "    example_selector= ex_selector,\n",
        "    example_prompt=ex_prompt,\n",
        "    prefix=\"Give the location an item is usually found in\",\n",
        "    suffix = \"Input: {noun}\\nOutput:\",\n",
        "    input_variables=[\"noun\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "gAqcKdgUZCaT",
        "outputId": "647b0774-1c49-420e-afcd-3d3765cd086b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Give the location an item is usually found in\\n\\nExample Input: driver\\nExample Output: car\\n\\nExample Input: pilot\\nExample Output: plane\\n\\nInput: student\\nOutput:'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "similar_prompt.format(noun = \"student\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "083KW27bZLft",
        "outputId": "26e92252-79ce-44e7-8fae-e3f62dd5088e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' classroom'"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm(similar_prompt.format(noun='student'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovhue4QpZnM3"
      },
      "source": [
        "#### Output Parsers\n",
        "- A helpful way to format the output of a model. Usually used for structured output.\n",
        "1. Format Instructions\n",
        "- A autogenerated prompt that tells the LLM how to format its response based off your desired result.\n",
        "2. Parser\n",
        "- A method which will extract your model's text output into a desired structure (usually json)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "hYXQu45UaLIA"
      },
      "outputs": [],
      "source": [
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain.llms import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "fkKo4Ienah9K"
      },
      "outputs": [],
      "source": [
        "response_schemas = [\n",
        "    ResponseSchema(name=\"bad_string\", description=\"This a poorly formatted user input string\"),\n",
        "    ResponseSchema(name=\"good_string\", description=\"This is your response, a reformatted response\")\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "VMAsV5AZarga"
      },
      "outputs": [],
      "source": [
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "7vH-7exCa50K",
        "outputId": "145feabf-3c83-4bc5-faf7-d97a11a77b16"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"bad_string\": string  // This a poorly formatted user input string\\n\\t\"good_string\": string  // This is your response, a reformatted response\\n}\\n```'"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "format_instructions = output_parser.get_format_instructions()\n",
        "format_instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "DJuy4iWebHM7"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"\n",
        "You will be given a poorly formatted string from a user.\n",
        "Reformat it and make sure all the words are spelled correctly\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "% USER INPUT:\n",
        "{user_input}\n",
        "\n",
        "YOUR RESPONSE:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "Rj2IF_iSbeib"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(input_variables=['user_input'], partial_variables={\"format_instructions\":format_instructions}, template = template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "MhzuikRwbvwp"
      },
      "outputs": [],
      "source": [
        "prompt_value = prompt.format(user_input = \"welcom to califonya!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "aDfDTW-ab52L"
      },
      "outputs": [],
      "source": [
        "output = llm(prompt_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mzHkRGQcDIq",
        "outputId": "458de698-b42b-4d93-8419-50cb68cd26f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'bad_string': 'welcom to califonya!', 'good_string': 'Welcome to California!'}"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_parser.parse(output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
